{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d03cba-d16b-4fe4-a991-a8755a654ca0",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19291d3-9223-44b1-900a-2496025e13de",
   "metadata": {},
   "source": [
    "NOTATIONS\n",
    "\n",
    "To denote different features we will denote x_1 x_2 etc. <br>\n",
    "x subscript j will denote features <br>\n",
    "n = number of features <br>\n",
    "x superscript i will be a vector that includes all the features of ith training example i.e basically a row ..it is row vector...it is a list of vectors <br>\n",
    "x subscript j superscript i means a particular value i.e single element <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a5be2-ac01-450b-99cb-7d358e719535",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Previously f(x) = wx + b.\n",
    "Now,\n",
    "\n",
    "`f(x) = w1x1 + w2x2 + w3x3 + w4x4 + ... + b`\n",
    "\n",
    "w = [w1 w2 w3...]  #It is a row vector <br>\n",
    "b is a number <br>\n",
    "x = [x1 x2 x3 ...]  #It is list of features , row vectors <br>\n",
    "\n",
    "`f(x) = w.x + b` <br>\n",
    "Where there is dot product between w and x vectors here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bca6f-99b8-40b7-bdc9-fbf42fee5ed4",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "You're implementing a learning algorithm, using vectorization will both make your code shorter and also make it run much more efficiently. GPU hardware that stands for graphics processing unit. This is hardware objectively designed to speed up computer graphics in your computer, but turns out can be used when you write vectorized code to also help you execute your code much more quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff50a512-167e-405d-8ece-8b08bb8b724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f765847-ebf7-4ba6-b7bf-92a5ce2a874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0,2.5,-3.3])\n",
    "b = 4\n",
    "x = np.array([10,20,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f092cd-f412-4a94-85c1-b5669d032823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without vectorization implementation\n",
    "f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b\n",
    "#but when n = 100 then very lengthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75cbaf67-37f4-40d4-bfa4-427c26183ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can also use loop but this is still without vectorization\n",
    "#here basically all multiplications of w and x par summition is done and then at last b is added\n",
    "f = 0\n",
    "n = 3 #kitne features hai\n",
    "for j in range(0,n): #means loop goes from 0 to n-1 or u can also write rang(n)\n",
    "    f = f + w[j] * x[j]\n",
    "f = f + b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45455e59-d042-4005-9ab5-122ac2e757c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with vectorization its done in single line by using the dot product from maths\n",
    "f = np.dot(w,x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3faed-8cd0-4431-b912-4bbf963269e0",
   "metadata": {},
   "source": [
    "Vectorization actually has two distinct benefits. First, it makes code shorter, is now just one line of code. Second, it also results in your code running much faster than either of the two previous implementations that did not use vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b1470-62b8-4951-8007-519dc3e631f0",
   "metadata": {},
   "source": [
    "### What do computers do in background to run the vectorization code run faster \n",
    "\n",
    "In for loop if the range is 0 to 15 then it performes operations one after another. It at first timestamp calculates at index 0. f + w[0] * x[0]. The at second timestamp calculates the value at index 1 and so on till 15th step.In other words, it calculates these computations one step at a time, one step after another. <br>\n",
    "In contrast, this function in NumPy is implemented in the computer hardware with vectorization. The computer can get all values of the vectors w and x, and in a single-step, it multiplies each pair of w and x with each other all at the same time in parallel. Then after that, the computer takes these 16 numbers and uses specialized hardware to add them altogether very efficiently, rather than needing to carry out distinct additions one after another to add up these 16 numbers. This means that codes with vectorization can perform calculations in much less time than codes without vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5efdee-2690-4822-91e8-22b37520ec40",
   "metadata": {},
   "source": [
    "When we will do gradient descent without vectorization, we have to multiply each derivative term with learning rate and find each w. <br>\n",
    "w1 = w1 - 0.1*d1 <br>\n",
    "w2 = w2 - 0.1*d2 <br>\n",
    "...so on <br>\n",
    "With Vectorization <br>\n",
    "w = w - 0.1*d where w and d are vectors with arrows <br>\n",
    "What it does....all values of w in parallel ...substracts 0.1 times all values of d from respective w and assign all 16 values in code in 1 step. <br>\n",
    "`HENCE PARALLEL PROCESSING HARDWARE IS USED HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1c0dd-490c-4489-8395-9de82d575666",
   "metadata": {},
   "source": [
    "## Python, NumPy, Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54fe22d-d592-4125-99c0-a654d6fa7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e38ef-08b7-421b-a8b7-cfb0667a013f",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "The elements of a vector are all the same type. The number of elements in the array is often referred to as the dimension though mathematicians may prefer rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3d20f-f0ae-42fe-b2e0-10ec581b99c3",
   "metadata": {},
   "source": [
    "#### Vector Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59821db6-0f82-4f04-8cc6-4025f39dc604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4), a = [0. 0. 0. 0.] shape = (4,) , data type = float64\n",
      "np.zeros((4,)), a = [0. 0. 0. 0.] shape = (4,) , data type = float64\n",
      "np.random.random_sample(4), a = [0.1679925  0.7289732  0.60115897 0.49925292] shape = (4,) , data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(4)\n",
    "print(f\"np.zeros(4), a = {a} shape = {a.shape} , data type = {a.dtype}\")\n",
    "a = np.zeros((4,))\n",
    "print(f\"np.zeros((4,)), a = {a} shape = {a.shape} , data type = {a.dtype}\")\n",
    "a = np.random.random_sample((4))\n",
    "print(f\"np.random.random_sample(4), a = {a} shape = {a.shape} , data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4184b86-a0e8-4883-8b3d-d091c0afbec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0. 1. 2. 3.] , (4,) , float64\n",
      "[0.9849147  0.99861715 0.17869075 0.98629556] (4,) float64\n"
     ]
    }
   ],
   "source": [
    "#these dont take shape as input\n",
    "a = np.arange(4.) #4. gives floating values\n",
    "print(f\" {a} , {a.shape} , {a.dtype}\")\n",
    "a = np.random.rand(4)\n",
    "print(a , a.shape, a.dtype)\n",
    "#the only diff in rand and random_sample is in random_sample arg is tuple i.e ((4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84d60f32-9f17-4902-9e5e-6f7d629c7d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int32\n",
      "np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "#(4,) indicates 1d array with 4 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cacd1-832c-4b4b-9423-a57f11d36702",
   "metadata": {},
   "source": [
    "#### Operations on Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fd6cd-ae6f-45e8-8a22-f1087235c222",
   "metadata": {},
   "source": [
    "##### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f0d17-5e8b-493d-9e84-c13fdb8ff7b9",
   "metadata": {},
   "source": [
    "Indexing means referring to an element of an array by its position within the array.\n",
    "Slicing means getting a subset of elements from an array based on their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30e739b6-5258-456b-bd3f-d8c6409000b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      " () 2 accessing an element return scalar\n",
      "a[-1] = 9\n",
      "Error:\n",
      "index 10 is out of bounds for axis 0 with size 10\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "print(f\" {a[2].shape} {a[2]} accessing an element return scalar\")\n",
    "print(f\"a[-1] = {a[-1]}\")\n",
    "\n",
    "try:\n",
    "    c = a[10]\n",
    "except Exception as e: #if any exception caught then assing it to e\n",
    "    print(\"Error:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23889ae1-2278-433d-b40b-9acf237158b6",
   "metadata": {},
   "source": [
    "##### Slicing\n",
    "\n",
    "Slicing creates an array of indices using a set of three values(start:stop:step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b24cb91-de2f-455f-a495-57aa91d11f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a         = [0 1 2 3 4 5 6 7 8 9]\n",
      "[2 3 4 5 6]\n",
      "[2 4 6]\n",
      "[3 4 5 6 7 8 9]\n",
      "[0 1 2]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "print(f\"a         = {a}\")\n",
    "\n",
    "c = a[2:7:1]\n",
    "print(c)\n",
    "\n",
    "c = a[2:7:2]\n",
    "print(c)\n",
    "\n",
    "c = a[3:]\n",
    "print(c)\n",
    "\n",
    "c = a[:3]\n",
    "print(c)\n",
    "\n",
    "c = a[:]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42132316-9f95-4ba9-bb60-d65c2e36b3e8",
   "metadata": {},
   "source": [
    "##### Single vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e876e86-cbbd-4085-9fce-f15b3bb419f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a             : [1 2 3 4]\n",
      "b = -a        : [-1 -2 -3 -4]\n",
      "b = np.sum(a) : 10\n",
      "b = np.mean(a): 2.5\n",
      "b = a**2      : [ 1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "print(f\"a             : {a}\")\n",
    "\n",
    "b = -a \n",
    "print(f\"b = -a        : {b}\")\n",
    "\n",
    "b = np.sum(a) \n",
    "print(f\"b = np.sum(a) : {b}\")\n",
    "\n",
    "b = np.mean(a)\n",
    "print(f\"b = np.mean(a): {b}\")\n",
    "\n",
    "b = a**2\n",
    "print(f\"b = a**2      : {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca59d9d-2416-4f41-af1b-125587bddc1d",
   "metadata": {},
   "source": [
    "##### Vector Vector element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52d7bf16-db89-408b-8cb7-48807c366678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary operators work element wise: [0 0 6 8]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([ 1, 2, 3, 4])\n",
    "b = np.array([-1,-2, 3, 4])\n",
    "print(f\"Binary operators work element wise: {a + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "754a77e1-73cd-4e5e-ba0e-98c4dbc2507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error message you'll see is:\n",
      "operands could not be broadcast together with shapes (4,) (2,) \n"
     ]
    }
   ],
   "source": [
    "#try a mismatched vector operation\n",
    "c = np.array([1, 2])\n",
    "try:\n",
    "    d = a + c\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bdf45-40f0-4990-89db-113ab5f2f47c",
   "metadata": {},
   "source": [
    "##### Scalar Vector operations\n",
    "\n",
    "Vectors can be 'scaled' by scalar values. A scalar value is just a number. The scalar multiplies all the elements of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a5d588b-df32-44a8-836f-04a00e90cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = 5 * a : [ 5 10 15 20]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "\n",
    "# multiply a by a scalar\n",
    "b = 5 * a \n",
    "print(f\"b = 5 * a : {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3388fbf-dbc7-42ae-8986-556003aeb567",
   "metadata": {},
   "source": [
    "##### Vector Vector dot product\n",
    "\n",
    "The dot product multiplies the values in two vectors element-wise and then sums the result. Vector dot product requires the dimensions of the two vectors to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92f6153c-ae58-4d05-88f4-9d85969d7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using for loop first\n",
    "def my_dot(a,b):\n",
    "    x = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        x = x +  a[i] * b[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7694ce1-b737-4f0b-89f7-9eb8b436c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "#test 1D\n",
    "a = np.array([1,2,3,4])\n",
    "b = np.array([-1,4,3,2])\n",
    "print (f\"{my_dot(a,b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d42fdd97-5578-40aa-9bce-0ef707c630ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#try using np.dot\n",
    "a = np.array([1,2,3,4])\n",
    "b = np.array([-1,4,3,2])\n",
    "c = np.dot(a,b)\n",
    "d = np.dot(b,a)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf6121-8104-497f-9672-d2d67d35fc2d",
   "metadata": {},
   "source": [
    "##### The Need for Speed: vector vs for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "751452e1-929f-479a-b652-c4c4bc7f400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501072.5817\n",
      "27.1971ms\n",
      "2501072.5816813707\n",
      "5646.1115ms\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#Setting a seed ensures reproducibility â€” every time you run the code, it will produce the same sequence of random numbers.\n",
    "a = np.random.rand(10000000) #very large arrays\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time()\n",
    "c  = np.dot(a,b)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"{c:.4f}\")\n",
    "print(f\"{1000*(toc-tic):.4f}ms\")\n",
    "\n",
    "tic = time.time()\n",
    "c = my_dot(a,b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(f\"{1000*(toc-tic):.4f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f4ece-0cad-4e51-a40a-6e98ac1acc32",
   "metadata": {},
   "source": [
    "\n",
    "So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU's and modern CPU's implement the Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83792d06-d4db-446f-97f0-4f123997a6a1",
   "metadata": {},
   "source": [
    "##### Vector Vector operations in Course 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f746d8b4-f6cb-4ef3-88eb-b29c7db0b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[1] has shape (1,)\n",
      "X[1] has shape (4, 1)\n",
      "w has shape (1,) , [2]\n",
      "c has shape () , 4\n"
     ]
    }
   ],
   "source": [
    "# show common Course 1 example\n",
    "X = np.array([[1],[2],[3],[4]])\n",
    "w = np.array([2])\n",
    "c = np.dot(X[1], w)\n",
    "\n",
    "print(f\"X[1] has shape {X[1].shape}\")\n",
    "print(f\"X[1] has shape {X.shape}\")\n",
    "print(f\"w has shape {w.shape} , {w}\")\n",
    "print(f\"c has shape {c.shape} , {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cadfab-7eb0-4814-a957-de2f2bddf8cc",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007fb1f-09bc-430a-a06f-359a21bc8007",
   "metadata": {},
   "source": [
    "m = Number of rows <br>\n",
    "n = Number of columns <br>\n",
    "Course 1 does not do operations directly on matrices but typically extracts an example as a vector and operates on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cd048-65d8-49f6-89ea-a446adb64781",
   "metadata": {},
   "source": [
    "#### Creation\n",
    "\n",
    "Notice how NumPy uses brackets to denote each dimension. Notice further than NumPy, when printing, will print one row per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a3c8b1d-5e73-4fbb-b21e-d74ef60374b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape = (1, 5), a = [[0. 0. 0. 0. 0.]]\n",
      "a shape = (2, 1), a = [[0.]\n",
      " [0.]]\n",
      "a shape = (1, 1), a = [[0.44236513]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1, 5))                                       \n",
    "print(f\"a shape = {a.shape}, a = {a}\")                     \n",
    "\n",
    "a = np.zeros((2, 1))                                                                   \n",
    "print(f\"a shape = {a.shape}, a = {a}\") \n",
    "\n",
    "a = np.random.random_sample((1, 1))  \n",
    "print(f\"a shape = {a.shape}, a = {a}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1589ab5c-cf9a-4f86-b879-ec4ec6de0bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a shape = (3, 1), np.array: a = [[5]\n",
      " [4]\n",
      " [3]]\n",
      " a shape = (3, 1), np.array: a = [[5]\n",
      " [4]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[5], [4], [3]]);   print(f\" a shape = {a.shape}, np.array: a = {a}\")\n",
    "a = np.array([[5],   # One can also\n",
    "              [4],   # separate values\n",
    "              [3]]); #into separate rows\n",
    "print(f\" a shape = {a.shape}, np.array: a = {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce651e-5067-4bec-9fd4-08ea6ebd56cf",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c12acf-dced-403e-a2dd-8522cd7ae9f7",
   "metadata": {},
   "source": [
    "##### Indexing \n",
    "\n",
    "\n",
    "Matrices include a second index. The two indexes describe [row, column]. Access can either return an element or a row/column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f471dac1-ec4b-4a22-a36f-e35d5ed0d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: (3, 2), \n",
      "a= [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "\n",
      "a[2,0].shape:   (), a[2,0] = 4,     type(a[2,0]) = <class 'numpy.int32'> Accessing an element returns a scalar\n",
      "\n",
      "a[2].shape:   (2,), a[2]   = [4 5], type(a[2])   = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# -1 means you compute this dimension automatically, based on the size of the array and the other dimensions I provide.\n",
    "# or we can say The -1 argument tells the routine to compute the number of rows given the size of the array and the number of columns.\n",
    "a = np.arange(6).reshape(-1, 2)\n",
    "print(f\"a.shape: {a.shape}, \\na= {a}\")\n",
    "\n",
    "#access an element\n",
    "print(f\"\\na[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\\n\")\n",
    "\n",
    "#access a row\n",
    "print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb2c52-ddd4-4111-92bb-f63c68d0497e",
   "metadata": {},
   "source": [
    "##### Slicing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd3008d9-307e-43d0-85c5-e1df83a4b7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n",
      "a[0, 2:7:1] =  [2 3 4 5 6] ,  a[0, 2:7:1].shape = (5,) a 1-D array\n",
      "a[:, 2:7:1] = \n",
      " [[ 2  3  4  5  6]\n",
      " [12 13 14 15 16]] ,  a[:, 2:7:1].shape = (2, 5) a 2-D array\n",
      "a[:,:] = \n",
      " [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]] ,  a[:,:].shape = (2, 10)\n",
      "a[1,:] =  [10 11 12 13 14 15 16 17 18 19] ,  a[1,:].shape = (10,) a 1-D array\n",
      "a[1]   =  [10 11 12 13 14 15 16 17 18 19] ,  a[1].shape   = (10,) a 1-D array\n"
     ]
    }
   ],
   "source": [
    "#start : stop : step\n",
    "a = np.arange(20).reshape(-1, 10)\n",
    "print(f\"a = \\n{a}\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step)\n",
    "print(\"a[0, 2:7:1] = \", a[0, 2:7:1], \",  a[0, 2:7:1].shape =\", a[0, 2:7:1].shape, \"a 1-D array\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step) in two rows\n",
    "print(\"a[:, 2:7:1] = \\n\", a[:, 2:7:1], \",  a[:, 2:7:1].shape =\", a[:, 2:7:1].shape, \"a 2-D array\")\n",
    "\n",
    "# access all elements\n",
    "print(\"a[:,:] = \\n\", a[:,:], \",  a[:,:].shape =\", a[:,:].shape)\n",
    "\n",
    "# access all elements in one row (very common usage)\n",
    "print(\"a[1,:] = \", a[1,:], \",  a[1,:].shape =\", a[1,:].shape, \"a 1-D array\")\n",
    "# same as\n",
    "print(\"a[1]   = \", a[1],   \",  a[1].shape   =\", a[1].shape, \"a 1-D array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf0634-1cf6-49ae-a5a5-fccda952755f",
   "metadata": {},
   "source": [
    "### Gradient Descent for multiple linear regression\n",
    "\n",
    "You've learned about gradient descents about multiple linear regression and also vectorization. Let's put it all together to implement gradient descent for multiple linear regression with vectorization. <br>\n",
    "Explained the gradient descent for multiple features. <br>\n",
    "`AN ALTERNATIVE TO GRADIENT DESCENT` <br>\n",
    "This method is called normal equation. Runs only for linear regression. This method is used to find w and b and doesnt need gradient descent. It turns out to be possible to use an advanced linear algebra library to just solve for w and b all in one goal without iterations. Slow when numbers of features is large. `If you're using a mature machine learning library and call linear regression, there is a chance that on the backend, it'll be using this to solve for w and b.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5196e40-effe-4084-b5c9-6057110fedbc",
   "metadata": {},
   "source": [
    "### Implementing Multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbec2785-e114-4ed7-a9cf-808e60e91449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy,math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"C:/Users/prish/Python/Machine_Learning/deeplearning.mplstyle\")\n",
    "np.set_printoptions(precision=2) #reduced display precision on numpy arrays\n",
    "#Show only 2 digits after the decimal point for floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15ebf568-a2c0-4522-ac6b-bf4c30b25000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a non bold is scalar whereas bold is vector\n",
    "# A is matrix\n",
    "# m is number of training examples basically rows\n",
    "# n is number of features in each example nasically columns\n",
    "\n",
    "# our training dataset  has 4 features and 3 examples and size is in sqft and not 1000 sqft\n",
    "X_train = np.array([[1204,5,1,45],[1416,3,2,40],[852,22,1,35]])\n",
    "Y_train = np.array([460,232,178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6211c55-9525-48ee-a1d4-fa34ea50799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (3, 4), X type:<class 'numpy.ndarray'>\n",
      "[[1204    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852   22    1   35]]\n",
      "Y_shape: (3,), Y type:<class 'numpy.ndarray'>\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "#data is stored in numpy array/matrix\n",
    "print(f\"X_shape: {X_train.shape}, X type:{type(X_train)}\")\n",
    "print(X_train)\n",
    "print(f\"Y_shape: {Y_train.shape}, Y type:{type(Y_train)}\")\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "529a85b7-7a74-40f8-8ca4-b60f27982293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape : (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "# w and b will be loaded with some initial selected values that are near the optimal\n",
    "print(f\"w_init shape : {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e3664-cab9-4310-a6ce-4fabdfcc68d4",
   "metadata": {},
   "source": [
    "#### Single Prediction element by element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8a281-b9e7-4939-a3fd-c6ffe8873468",
   "metadata": {},
   "source": [
    "A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b047e93-cd2b-497d-9a7d-d2c99027e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x,w,b): #we are finding cost here\n",
    "    '''\n",
    "    Single predict using linear regression\n",
    "    Args:\n",
    "        x (ndarray): Shape (n,) example with multiple features\n",
    "        w (ndarray): Shape (n,) model parameters    \n",
    "        b (scalar):  model parameter\n",
    "    Retuns:\n",
    "        p(scalar) : prediction\n",
    "    '''\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]\n",
    "        p = p + p_i\n",
    "    p = p + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b55f4288-e002-46d7-9c49-87ba1de8bb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape : (4,), x_vec value: [1204    5    1   45]\n",
      "107.79818261940818\n"
     ]
    }
   ],
   "source": [
    "#get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape : {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "#make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f_wb)\n",
    "#u cant apply .shape on float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726886f-a1ed-4f3b-82d9-5ae991bc4530",
   "metadata": {},
   "source": [
    "#### Making use of Dot function to make our code smaller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11079616-24c3-4c1d-8e50-9f15acaa4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w,b):\n",
    "    p = np.dot(x,w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b86ef512-82c8-4ad8-aa47-5677e6a3ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value : [1204    5    1   45]\n",
      "() 107.79818261940818\n"
     ]
    }
   ],
   "source": [
    "#now doing the prediction\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value : {x_vec}\")\n",
    "\n",
    "#make a prediction\n",
    "f_wb = predict(x_vec, w_init, b_init)\n",
    "print(f_wb.shape , f_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4dcdc-965e-4d1e-9350-4301c28fa3e0",
   "metadata": {},
   "source": [
    "The results from both are same and going forward np.dot will be used for these operations. The prediction is now single statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f6c2b-bf0e-43b5-89a4-c46ca90d2ba0",
   "metadata": {},
   "source": [
    "#### Computing Cost with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ac34658-3bd2-425d-a7e4-6e7df243afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,y,w,b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"   \n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        #yaha par x[i] gives a particular row with all features and x[i],w ka dot product is directly giving the answer of multiplying all features with their weights\n",
    "        f_wb_i = np.dot(x[i],w) + b\n",
    "        cost = cost + (f_wb_i - y[i])**2\n",
    "    cost = cost / (2*m)\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9033b4b5-e8f8-4d71-b68c-e0daf0ef7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 44121.272705907795\n"
     ]
    }
   ],
   "source": [
    "#compute and display cost using our pre-chosen optimal parameters\n",
    "cost = compute_cost(X_train,Y_train,w_init,b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a558e-653f-484b-bbdf-8545917821fb",
   "metadata": {},
   "source": [
    "#### Gradient with multiple variables\n",
    "\n",
    "Here there is an outer loop over all m examples. dj_db for all examples can be computed directly. <br>\n",
    "in second loop over all n features. dj_dw_j is computed for each w_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00002198-0861-4a50-bb22-4a904cda7201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    '''\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "        x: Data, m examples with n features\n",
    "        y: Target values\n",
    "        x: model parameters\n",
    "        b: model parameter\n",
    "    Returns:\n",
    "    dj_dw and dj_db\n",
    "    '''\n",
    "    m,n = x.shape #number of examples and number of features\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(x[i],w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*x[i,j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7e3a2ef-f130-485d-b3dd-937e91e33ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.6245093927415155\n",
      "\n",
      " [-3.48e+04  2.16e+03  7.62e+00 -9.07e+02]\n"
     ]
    }
   ],
   "source": [
    "#compute and display gradient\n",
    "tmp_dj_db,tmp_dj_dw = compute_gradient(X_train,Y_train,w_init,b_init)\n",
    "print(tmp_dj_db)\n",
    "print('\\n' , tmp_dj_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecbc57e-5918-453d-889f-12052fca3b56",
   "metadata": {},
   "source": [
    "#### Gradient Descent with multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "997d482d-7d57-42b3-8333-7adb8251c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w_in,b_in,cost_function,gradient_function, alpha, num_iters):\n",
    "    '''\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    Returns:\n",
    "        w : updated values of parameters\n",
    "        b : updated value of paramter\n",
    "        J_hist : cost value history\n",
    "    '''\n",
    "    J_hist = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_db,dj_dw = gradient_function(x,y,w,b)\n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha *  dj_db\n",
    "\n",
    "        if i<100000:\n",
    "            J_hist.append(cost_function(x,y,w,b))\n",
    "\n",
    "        if i%math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d} : Cost {J_hist[-1]:8.2f} \")\n",
    "\n",
    "    return w,b,J_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "102f8cd5-6589-4b8b-a982-13654c987149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 : Cost 10788.03 \n",
      "Iteration  100 : Cost  6877.80 \n",
      "Iteration  200 : Cost  6866.15 \n",
      "Iteration  300 : Cost  6854.52 \n",
      "Iteration  400 : Cost  6842.92 \n",
      "Iteration  500 : Cost  6831.35 \n",
      "Iteration  600 : Cost  6819.81 \n",
      "Iteration  700 : Cost  6808.30 \n",
      "Iteration  800 : Cost  6796.81 \n",
      "Iteration  900 : Cost  6785.36 \n",
      "b,w found : 0.00,[ 0.24 -0.04 -0.02  0.24]\n",
      "Prediction: 299.06 , target value : 460\n",
      "Prediction: 348.68 , target value : 232\n",
      "Prediction: 211.57 , target value : 178\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "w_final,b_final,J_hist = gradient_descent(X_train, Y_train,initial_w,initial_b,compute_cost,compute_gradient,alpha,iterations)\n",
    "print(f\"b,w found : {b_final:0.2f},{w_final}\")\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f'Prediction: {np.dot(X_train[i],w_final) + b_final:0.2f} , target value : {Y_train[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5a202dba-bd49-49e1-a8b4-2d648f99d275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'iteration step')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a615-cf03-45a0-86a3-c13cb6809af1",
   "metadata": {},
   "source": [
    "These results are not inspiring! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2115540-d64a-42c5-8a1a-c9cf47779545",
   "metadata": {},
   "source": [
    "### Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ace6f7-0841-4731-9f98-a3ddb244ae70",
   "metadata": {},
   "source": [
    "You see a technique called feature scaling that will enable gradient descent to run much faster. Notice that when a possible range of values of a feature is large, like the size and square feet which goes all the way up to 2000. It's more likely that a good model will learn to choose a relatively small parameter value, like 0.1. Likewise, when the possible values of the feature are small, like the number of bedrooms, then a reasonable value for its parameters will be relatively large like 50.\n",
    "\n",
    "In Scatter plot for features where size is on x axis and bedrooms on y axis, in that the points will lie mostly in lower part of graph above x since bedrooms ranges between 0-5 but it would be scattered all over the x axis since size range from 300-2000. <br>\n",
    "The contours form ovals or ellipses and they're short on one side and longer on the other. And this is because a very small change to w1 can have a very large impact on the estimated price and that's a very large impact on the cost J. Because w1 tends to be multiplied by a very large number, the size and square feet. In contrast, it takes a much larger change in w2 in order to change the predictions much. And thus small changes to w2, don't change the cost function nearly as much.\n",
    "![Contour Plot](Contour_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63696ed-1742-4d7f-8150-380a32885466",
   "metadata": {},
   "source": [
    "Because the contours are so tall and skinny gradient descent may end up bouncing back and forth for a long time before it can finally find its way to the global minimum. In situations like this, a useful thing to do is to scale the features. This means performing some transformation of your training data so that x1 say might now range from 0 to 1 and x2 might also range from 0 to 1. So the data points now look more like this and you might notice that the scale of the plot on the bottom is now quite different than the one on top.\n",
    "![Scatter_Plot](Scatter_Plot.png) <br>\n",
    "Now the the contours will look more like more like circles and less tall and skinny. And gradient descent can find much more direct path to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5961a3-d6d6-4520-94aa-0e1902dddcb2",
   "metadata": {},
   "source": [
    "So to recap, when you have different features that take on very different ranges of values, it can cause gradient descent to run slowly but re scaling the different features so they all take on comparable range of values. because speed, upgrade and dissent significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85423b-2af8-4e76-b45d-e8bd5d3ed2c4",
   "metadata": {},
   "source": [
    "METHODS OF FEATURE SCALING\n",
    "\n",
    "1. Dividing by maximum <br>\n",
    "Suppose we have x1 ranges from 300 to 2000. Then one method to scale is to divide all original values by 2000 i.e maximum of range. And hence the range of x1 becomes now between 0.15 to 1. X2 ranges from 0 to 5 then u can have its scale version by dividing all values by maximum of range i.e 5. And it will now range from 0 to 1 <br>\n",
    "\n",
    "2. Mean Normalization <br>\n",
    "You start with the original features and then you re-scale them so that both of them are centered around zero. Whereas before they only had values greater than zero, now they have both negative and positive values that may be usually between negative one and plus one. <br>\n",
    "Find mean of each feature. Then <br>\n",
    " ` x1 = (x1-mean)/(max-min)` <br>\n",
    "Now x1 will range between -0.18 to 0.82. Similarly for x2. <br>\n",
    "\n",
    "3. Z-score normalization\n",
    "Find mean and sd for each feature. <br>\n",
    "`x1 = (x1-mean)/sd` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cb2a4-78c6-4c73-80c0-bbf9fc1364ee",
   "metadata": {},
   "source": [
    "Mostly we try to bring the range between -1 to 1 but sometimes it is acceptable to have range between -3 to 3 or -0.3 to 0.3 or 0 to 3. Then its totally your wish whether to rescale it or not. But if the values range between very large like -100 to 100 or very small like -0.001 to 0.001 then u have rescale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207e61e-666e-46bc-93d7-6cc11af3cf76",
   "metadata": {},
   "source": [
    "### Checking gradient descent for convergence\n",
    "\n",
    "1. Learning curve<br>\n",
    "What I'll often do is plot the cost function J, which is calculated on the training set, and I plot the value of J at each iteration of gradient descent. \n",
    "![Learning_Curve](Learning_curve.png) <br>\n",
    "If gradient descent is working properly, then the cost J should decrease after every single iteration. If J ever increases after one iteration, that means either Alpha is chosen poorly, and it usually means Alpha is too large, or there could be a bug in the code. By 400 iterations, it looks like the curve has flattened out. This means that gradient descent has more or less converged because the curve is no longer decreasing. It turns out to be very difficult to tell in advance how many iterations gradient descent needs to converge, which is why you can create a graph like this, a learning curve.\n",
    "\n",
    "2. Automatic convergence test <br>\n",
    "Let's let epsilon be a variable representing a small number, such as 0.001 or 10^-3. If the cost J decreases by less than this number epsilon on one iteration, then you're likely on this flattened part of the curve that you see on the left and you can declare convergence.I usually find that choosing the right threshold epsilon is pretty difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef628629-26f7-4fba-b20a-e595d079e600",
   "metadata": {},
   "source": [
    "### Choosing the learning rate\n",
    "\n",
    "So if gradient descent isn't working, one thing I often do and I hope you find this tip useful too, one thing I'll often do is just set Alpha to be a very small number and see if that causes the cost to decrease on every iteration. If even with Alpha set to a very small number, J doesn't decrease on every single iteration, but instead sometimes increases, then that usually means there's a bug somewhere in the code. Take alpha 0.001 or then 0.01 then 0.1. For each iteration run a handful of iterations and plot the cost function. Plot the cost function J as a function of the number of iterations and after trying a few different values, you might then pick the value of Alpha that seems to decrease the learning rate rapidly, but also consistently. <br>\n",
    "`After trying 0.001 , try 0.003 i.e increasing the rate by 3 folds. The 0.01 i.e again increasing the rate by 3 folds and so on.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9de02-547a-4f05-8d98-62dfecd1847a",
   "metadata": {},
   "source": [
    "## Implementing feature-scaling and learning rate (Multi-variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b082fb1-fa58-4af9-aa39-06d2e2cbe6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
