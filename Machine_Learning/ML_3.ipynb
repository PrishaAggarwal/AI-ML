{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d03cba-d16b-4fe4-a991-a8755a654ca0",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19291d3-9223-44b1-900a-2496025e13de",
   "metadata": {},
   "source": [
    "NOTATIONS\n",
    "\n",
    "To denote different features we will denote x_1 x_2 etc. <br>\n",
    "x subscript j will denote features <br>\n",
    "n = number of features <br>\n",
    "x superscript i will be a vector that includes all the features of ith training example i.e basically a row ..it is row vector...it is a list of vectors <br>\n",
    "x subscript j superscript i means a particular value i.e single element <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a5be2-ac01-450b-99cb-7d358e719535",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Previously f(x) = wx + b.\n",
    "Now,\n",
    "\n",
    "`f(x) = w1x1 + w2x2 + w3x3 + w4x4 + ... + b`\n",
    "\n",
    "w = [w1 w2 w3...]  #It is a row vector <br>\n",
    "b is a number <br>\n",
    "x = [x1 x2 x3 ...]  #It is list of features , row vectors <br>\n",
    "\n",
    "`f(x) = w.x + b` <br>\n",
    "Where there is dot product between w and x vectors here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bca6f-99b8-40b7-bdc9-fbf42fee5ed4",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "You're implementing a learning algorithm, using vectorization will both make your code shorter and also make it run much more efficiently. GPU hardware that stands for graphics processing unit. This is hardware objectively designed to speed up computer graphics in your computer, but turns out can be used when you write vectorized code to also help you execute your code much more quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff50a512-167e-405d-8ece-8b08bb8b724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f765847-ebf7-4ba6-b7bf-92a5ce2a874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0,2.5,-3.3])\n",
    "b = 4\n",
    "x = np.array([10,20,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f092cd-f412-4a94-85c1-b5669d032823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without vectorization implementation\n",
    "f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b\n",
    "#but when n = 100 then very lengthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75cbaf67-37f4-40d4-bfa4-427c26183ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can also use loop but this is still without vectorization\n",
    "#here basically all multiplications of w and x par summition is done and then at last b is added\n",
    "f = 0\n",
    "n = 3 #kitne features hai\n",
    "for j in range(0,n): #means loop goes from 0 to n-1 or u can also write rang(n)\n",
    "    f = f + w[j] * x[j]\n",
    "f = f + b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45455e59-d042-4005-9ab5-122ac2e757c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with vectorization its done in single line by using the dot product from maths\n",
    "f = np.dot(w,x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3faed-8cd0-4431-b912-4bbf963269e0",
   "metadata": {},
   "source": [
    "Vectorization actually has two distinct benefits. First, it makes code shorter, is now just one line of code. Second, it also results in your code running much faster than either of the two previous implementations that did not use vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b1470-62b8-4951-8007-519dc3e631f0",
   "metadata": {},
   "source": [
    "### What do computers do in background to run the vectorization code run faster \n",
    "\n",
    "In for loop if the range is 0 to 15 then it performes operations one after another. It at first timestamp calculates at index 0. f + w[0] * x[0]. The at second timestamp calculates the value at index 1 and so on till 15th step.In other words, it calculates these computations one step at a time, one step after another. <br>\n",
    "In contrast, this function in NumPy is implemented in the computer hardware with vectorization. The computer can get all values of the vectors w and x, and in a single-step, it multiplies each pair of w and x with each other all at the same time in parallel. Then after that, the computer takes these 16 numbers and uses specialized hardware to add them altogether very efficiently, rather than needing to carry out distinct additions one after another to add up these 16 numbers. This means that codes with vectorization can perform calculations in much less time than codes without vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5efdee-2690-4822-91e8-22b37520ec40",
   "metadata": {},
   "source": [
    "When we will do gradient descent without vectorization, we have to multiply each derivative term with learning rate and find each w. <br>\n",
    "w1 = w1 - 0.1*d1 <br>\n",
    "w2 = w2 - 0.1*d2 <br>\n",
    "...so on <br>\n",
    "With Vectorization <br>\n",
    "w = w - 0.1*d where w and d are vectors with arrows <br>\n",
    "What it does....all values of w in parallel ...substracts 0.1 times all values of d from respective w and assign all 16 values in code in 1 step. <br>\n",
    "`HENCE PARALLEL PROCESSING HARDWARE IS USED HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1c0dd-490c-4489-8395-9de82d575666",
   "metadata": {},
   "source": [
    "## Python, NumPy, Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54fe22d-d592-4125-99c0-a654d6fa7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e38ef-08b7-421b-a8b7-cfb0667a013f",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "The elements of a vector are all the same type. The number of elements in the array is often referred to as the dimension though mathematicians may prefer rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3d20f-f0ae-42fe-b2e0-10ec581b99c3",
   "metadata": {},
   "source": [
    "#### Vector Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59821db6-0f82-4f04-8cc6-4025f39dc604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4), a = [0. 0. 0. 0.] shape = (4,) , data type = float64\n",
      "np.zeros((4,)), a = [0. 0. 0. 0.] shape = (4,) , data type = float64\n",
      "np.random.random_sample(4), a = [0.53259891 0.60239312 0.48234097 0.3266008 ] shape = (4,) , data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(4)\n",
    "print(f\"np.zeros(4), a = {a} shape = {a.shape} , data type = {a.dtype}\")\n",
    "a = np.zeros((4,))\n",
    "print(f\"np.zeros((4,)), a = {a} shape = {a.shape} , data type = {a.dtype}\")\n",
    "a = np.random.random_sample((4))\n",
    "print(f\"np.random.random_sample(4), a = {a} shape = {a.shape} , data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4184b86-a0e8-4883-8b3d-d091c0afbec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0. 1. 2. 3.] , (4,) , float64\n",
      "[0.87472643 0.14599844 0.57579957 0.9640381 ] (4,) float64\n"
     ]
    }
   ],
   "source": [
    "#these dont take shape as input\n",
    "a = np.arange(4.) #4. gives floating values\n",
    "print(f\" {a} , {a.shape} , {a.dtype}\")\n",
    "a = np.random.rand(4)\n",
    "print(a , a.shape, a.dtype)\n",
    "#the only diff in rand and random_sample is in random_sample arg is tuple i.e ((4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84d60f32-9f17-4902-9e5e-6f7d629c7d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int32\n",
      "np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "#(4,) indicates 1d array with 4 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cacd1-832c-4b4b-9423-a57f11d36702",
   "metadata": {},
   "source": [
    "#### Operations on Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443975a-a70d-45d0-bd6a-ee747b862747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
